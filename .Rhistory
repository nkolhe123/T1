qnorm(0.975,0,1)
qnorm(c(0.025,0.975),0,1)
a <- c(1,2,3,4,5,6,7,8,9,10)
mean(a)
var(a)
sqrt(var(a))
median(a)
a <- c(1,2,3,4,5,6,7,8,9,20)
mean(a)
var(a)
sqrt(var(a))
median(a)
a <- c(1,2,3,4,5,6,7,8,9,100)
var(a)
mean(a)
sqrt(var(a))
median(a)
a <- c(1,2,3,4,5,6,7,8,9,10)
mean(a)
var(a)
sqrt(var(a))
median(a)
qt(0.025,0,1,1)
qt(0.025,0,1,4)
qt(0.025,0,1,14)
qt(0.025,14)
pt(-2.14,0,1,14)
pt(-2.14,0,1)
pt(-2.14,0,1)
pt(0.14,0,1)
pt(1.2,10,lower.tail = T)
qt(0.025,14)
pt(-2.14,14,lower.tail = T)
qnorm(0.025,0,1)
qnorm(-1.95,0,1,lower.tail = T)
pnorm(-1.95,0,1,lower.tail = T)
qt(0.025,14) # 14 is DF
pt(-2.14,14,lower.tail = T)
qt(0.975,14) # 14 is DF
pt(2.145,14,lower.tail = T)
1000+(-1.64*(5/10))
a1<- 1000+(-1.64*(5/10))
a2<- 1000+(1.64*(5/10))
a2
pnorm(a1,998,5/10,lower.tail = F)
pnorm(a1,998,5/10,lower.tail = F)
pnorm(a1,998,5/10,lower.tail = F) - pnorm(a2,998,5/10,lower.tail = F)
pnorm(-1.95,0,1,lower.tail = T)
ft(0.975,7,10) # 7,14 is DF
qf(0.025,7,10) # 7,14 is DF
qf(0.975,7,10) # 7,14 is DF
pf(3.95,7,14,lower.tail = T)
pf(3.95,7,14,lower.tail = F)
qf(0.975,7,10) # 7,14 is DF
pf(3.95,7,14,lower.tail = T)
qf(0.975,7,10) # 7,14 is DF
pf(3.949824,7,14,lower.tail = T)
1-.1
.1/2
(1-.1)+.1/2
qf(0.95,7,10) # 7,14 is DF
.1/2
qf(0.05,24,23) # 7,14 is DF
qf(0.95,24,23) # 7,14 is DF
qf(0.95,6,9) # 7,14 is DF
qf(0.05,6,9) # 7,14 is DF
qf(0.05,6,9) # 7,14 is DF
qf(0.95,6,9) # 7,14 is DF
qf(0.05,6,9) # 7,14 is DF
qf(0.05,6,9, lower.tail = T) # 7,14 is DF
qf(0.05,6,9, lower.tail = F) # 7,14 is DF
qf(0.05,6,9, lower.tail = T) # 7,14 is DF
qf(0.95,6,9, lower.tail = F) # 7,14 is DF
qf(0.95,6,9, lower.tail = T) # 7,14 is DF
pt(-7.6,24, lower.tail = T)
pt(-7.6,24, lower.tail = T)
pt(-7.6,24,lower.tail = T)
pt(-7.6,24,lower.tail = F)
qnorm(0.1020,0,1)
install.packages("twitteR")
library(twitteR)
install.packages("twitteR")
library(twitteR)
api_key <- '4TrCFhHSDkdpA11s7uw6BS9VL'
api_secret <- 'etj2qeBWIThn46JBXBCo07mLiSlDgyFZ4jRvwGLDWfaLhQIuHD'
access_token <- '1902625928-QyInJOhZFUo4B98wSDMDYd2KzvxuwOvXxARGrXW'
access_token_secret <- 'sG5UxgoABRLttHE1VhtXQCBjH6NBKdr1cP86C2SiRHYSK'
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
tweets <- searchTwitter('$appl',n=10, lang = 'en')
tweets
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
tweets <- searchTwitter('$appl',n=10, lang = 'en')
tweets
tweets
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
tweets <- searchTwitter('$appl',n=10, lang = 'en')
tweets
tweets
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
tweets <- searchTwitter('$appl',n=10, lang = 'en')
tweets
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
tweets <- searchTwitter('$IPL',n=10, lang = 'en')
tweets
tweets <- searchTwitter('$CSK',n=10, lang = 'en')
tweets
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
tweets <- searchTwitter('$CSK',n=10, lang = 'en')
tweets <- searchTwitter('$IndianPremiumLeague',n=10, lang = 'en')
tweets <- searchTwitter('$KaranatakElection',n=10, lang = 'en')
tweets <- searchTwitter('$DELL',n=10, lang = 'en')
tweets <- searchTwitter('$iphone',n=10, lang = 'en')
tweets <- searchTwitter('$ChennaiIPL',n=10, lang = 'en')
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
tweets <- searchTwitter('$Gopichand25',n=10, lang = 'en')
tweets
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
tweets <- searchTwitter('$IPL',n=10, lang = 'en')
tweets
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
tweets <- searchTwitter('$IPL2018',n=10, lang = 'en')
tweets
#Twitter
api_key <- '4TrCFhHSDkdpA11s7uw6BS9VL'
api_secret <- 'etj2qeBWIThn46JBXBCo07mLiSlDgyFZ4jRvwGLDWfaLhQIuHD'
access_token <- '1902625928-QyInJOhZFUo4B98wSDMDYd2KzvxuwOvXxARGrXW'
access_token_secret <- 'sG5UxgoABRLttHE1VhtXQCBjH6NBKdr1cP86C2SiRHYSK'
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
tweets <- searchTwitter('$IPL2018',n=10, lang = 'en')
tweets
if(!require(qdap)) {install.packages("qdap")} # ensure java up to date!
library(qdap)
library(tidytext)
library(tidyverse)
# IBM Q3 2016 analyst call transcript
ibm = readLines('https://raw.githubusercontent.com/sudhir-voleti/sample-data-sets/master/International%20Business%20Machines%20(IBM)%20Q3%202016%20Results%20-%20Earnings%20Call%20Transcript.txt')
if(!require(qdap)) {install.packages("qdap")} # ensure java up to date!
library(qdap)
library(tidytext)
library(tidyverse)
# IBM Q3 2016 analyst call transcript
ibm = readLines('https://raw.githubusercontent.com/sudhir-voleti/sample-data-sets/master/International%20Business%20Machines%20(IBM)%20Q3%202016%20Results%20-%20Earnings%20Call%20Transcript.txt')
# apply polarity() func from qdap to compute sentiment polarity
t1 = Sys.time()   # set timer
pol = qdap::polarity(ibm)   # Calc polarity from qdap dictionary
wc = pol$all[,2]                  # Word Count in each doc
valence = pol$all[,3]                 # average polarity score
pos.words  = pol$all[,4]                  # Positive words info
neg.words  = pol$all[,5]                  # Negative Words info
Sys.time() - t1  # how much time did the above take?
pol
head(qdap_outp)
pos.words
neg.word
neg.words
# juxtapose qdap output against text in corpus
qdap_outp = ibm %>%
data_frame() %>%
data.frame(., pol$all) %>%
select(text.var, wc, polarity, pos.words, neg.words)
head(qdap_outp)
head(pol$group)
# Positive words list, do ?dplyr::setdiff
positive_words = unique(setdiff(unlist(pos.words),"-"))
# Print all the positive words found in the corpus
print(positive_words)
# Negative words list
negative_words = unique(setdiff(unlist(neg.words),"-"))
print(negative_words)       # Print all neg words
sentiments  # over 27k words across the 3 lexica
sentiments %>%
filter(lexicon == "AFINN") %>%
head()
textdf = data_frame(text = ibm)   # convert to data frame
bing = get_sentiments("bing")   # put all of the bing sentiment dict into object 'bing'
bing     # view bing object
senti.bing = textdf %>%
mutate(linenumber = row_number()) %>%   # build line num variable
ungroup() %>%
unnest_tokens(word, text) %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment, index = linenumber %/% 1, sort = FALSE) %>%
mutate(method = "bing")    # creates a column with method name
senti.bing
bing_df = data.frame(senti.bing %>% spread(sentiment, n, fill = 0))
head(bing_df)
bing_pol = bing_df %>%
mutate(polarity = (positive - negative)) %>%   #create variable polarity = pos - neg
arrange(desc(polarity), index)    # sort by polarity
bing_pol %>%  head()
require(ggplot2)
# plotting running sentiment distribution across the analyst call
ggplot(bing_pol,
aes(index, polarity)) +
geom_bar(stat = "identity", show.legend = FALSE) +
labs(title = "Sentiment in IBM analyst call corpus",
x = "doc",
y = "Sentiment")
bing_word_counts <- textdf %>%
unnest_tokens(word, text) %>%
inner_join(bing) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts
bing_word_counts %>%
filter(n > 3) %>%
mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
ylab("Contribution to sentiment")
require(wordcloud)
# build wordcloud of commonest tokens
textdf %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
# get AFINN first
AFINN <- get_sentiments("afinn")
AFINN
# inner join AFINN words and scores with text tokens from corpus
senti.afinn = textdf %>%
mutate(linenumber = row_number()) %>%
ungroup() %>%
unnest_tokens(word, text) %>%
inner_join(AFINN) %>%    # returns only intersection of wordlists and all columns
group_by(index = linenumber %/% 1) %>%
summarise(sentiment = sum(score)) %>%
mutate(method = "afinn")
senti.afinn
data.frame(senti.afinn) %>% head()
# first construct and split bigrams into word1 and word2
ibm_bigrams_separated <- textdf %>%
unnest_tokens(bigram, text,
token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ")
ibm_bigrams_separated
# examine the most frequent bigrams whose first word is a sentiment word
senti_bigrams <- ibm_bigrams_separated %>%
# word1 is from bigrams and word from AFINN
inner_join(AFINN, by = c(word1 = "word")) %>%
ungroup()
senti_bigrams
# what if we want sentiment associated with proper words, not stopwords?
senti_bigrams_filtered = ibm_bigrams_separated %>%
# filter out stopwords from each word in bigram
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
# word1 is from bigrams and word from AFINN
inner_join(AFINN, by = c(word1 = "word")) %>%
ungroup()
senti_bigrams_filtered
# view nrc dict structure
nrc = get_sentiments("nrc")
nrc
senti.nrc = textdf %>%
mutate(linenumber = row_number()) %>%
ungroup() %>%
# word-tokenize & merge nrc sentiment words
unnest_tokens(word, text) %>%
inner_join(get_sentiments("nrc")) %>%
count(sentiment, index = linenumber %/% 1, sort = FALSE) %>%  # %/% gives quotient
mutate(method = "nrc")
senti.nrc %>% head()
# make a neat table out of the 8 emotion dimensions
a = data.frame(senti.nrc %>% spread(sentiment, n, fill = 0))
head(a)
ibm_joy = textdf %>%
unnest_tokens(word, text) %>%
inner_join(nrc) %>%
filter(sentiment == "joy") %>%
count(word, sort = TRUE)
ibm_joy %>% head()
datafile1 <- read.csv("C:/Users/nkohle/Documents/ISB/TERM1/Practicum/t/txt1.txt")
datafile2 <- read.csv("C:/Users/nkohle/Documents/ISB/TERM1/Practicum/t/txt2.txt")
datafile1
datafile2
length(datafile1)
length(datafile2)
datafile <- rbind(datafile1,datafile2)
dim(datafile1)
dim(datafile2)
datafile <- rbind(datafile1,datafile2)
dim(datafile)
write.csv(txt3, row.names = F)
write.csv(datafile, row.names = F)
write.csv(datafile, row.names = F)
dim(datafile)
length(datafile)
write.csv(datafile,'txt3.txt', row.names = F)
setwd("C:/Users/nkohle/Documents/ISB/TERM1/Practicum/t")
write.csv(datafile,'txt3.txt', row.names = F)
library(rvest)
library(stringr)
library(dplyr)
library(magrittr)
library("rvest")
library("XML")
library(rvest)
library(stringr)
library(dplyr)
library(magrittr)
library("rvest")
library("XML")
# IMDB Top 250 Movies
url = "https://www.firstpost.com/firstcricket/sports-news/ipl-2018-over-reliance-on-virat-kohli-ab-de-villiers-poor-death-bowling-sum-up-another-underwhelming-season-for-rcb-4476841.html"
page = read_html(url)
page
txt = gsub(")","",                          # Removing )
gsub("\\(","",                   # Removing (
html_text(                  # get text of HTML node
html_nodes(page,'.fullcontent_4476841 p')
)))
txt
xmlTreeParse(txt[[1]])
txt = gsub(")","",                          # Removing )
gsub("\\(","",                   # Removing (
html_text(                  # get text of HTML node
html_nodes(page,'.fullcontent_4476841 p')
)))
xmlTreeParse(txt[[1]])
txt1 = html_text(txt)
url = "https://www.firstpost.com/firstcricket/sports-news/ipl-2018-over-reliance-on-virat-kohli-ab-de-villiers-poor-death-bowling-sum-up-another-underwhelming-season-for-rcb-4476841.html"
page = read_html(url)
txt = html_nodes(page,'.fullcontent_4476841 p')
txt
xmlTreeParse(txt[[1]])
txt1 = html_text(txt)
length(txt1)
txt
txt1 = html_text(txt)
txt1
url = "https://www.youthkiawaaz.com/2018/04/abortive-effort-of-royal-challengers-bangalore/"
page = read_html(url)
page
txt = html_nodes(page,'.overlap-div p')
txt
url = "https://www.youthkiawaaz.com/2018/04/abortive-effort-of-royal-challengers-bangalore/"
page = read_html(url)
txt = html_nodes(page,'.overlap-div p')
# Check one node
xmlTreeParse(txt[[1]])
url = "https://www.youthkiawaaz.com/2018/04/abortive-effort-of-royal-challengers-bangalore/"
page = read_html(url)
txt = html_nodes(page,'.overlap-div p')
# Check one node
xmlTreeParse(txt[[1]])
txt2 = html_text(txt)
length(txt2)
write.csv(txt2,'txt2.txt', row.names = F)
url = "https://www.firstpost.com/firstcricket/sports-news/ipl-2018-over-reliance-on-virat-kohli-ab-de-villiers-poor-death-bowling-sum-up-another-underwhelming-season-for-rcb-4476841.html"
page = read_html(url)
txt = html_nodes(page,'.fullcontent_4476841 p')
# Check one node
xmlTreeParse(txt[[1]])
txt1 = html_text(txt)
length(txt1)
write.csv(txt1,'txt1.txt', row.names = F)
url = "https://www.business-standard.com/article/ipl/ipl-2018-rcb-stop-chasing-the-target-ain-t-your-cup-of-tea-anymore-118041800467_1.html"
page = read_html(url)
txt = html_nodes(page,'.p-content p')
# Check one node
xmlTreeParse(txt[[1]])
txt2 = html_text(txt)
length(txt2)
write.csv(txt3,'txt3.txt', row.names = F)
txt3 = html_text(txt)
length(txt3)
write.csv(txt3,'txt3.txt', row.names = F)
url = "https://www.thequint.com/sports/cricket/ipl-2018-what-went-wrong-for-royal-challengers-bangalore"
page = read_html(url)
txt = html_nodes(page,'.story-article__content__element--text p')
# Check one node
xmlTreeParse(txt[[1]])
txt4 = html_text(txt)
length(txt4)
write.csv(txt4,'txt4.txt', row.names = F)
url = "https://www.quora.com/Can-RCB-win-IPL-2018"
page = read_html(url)
txt = html_nodes(page,'#__w2_nUdkhKG_paged_list_wrapper')
txt
url = "https://www.quora.com/Can-RCB-win-IPL-2018"
page = read_html(url)
txt = html_nodes(page,'#__w2_nUdkhKG_paged_list_wrapper')
txt
xmlTreeParse(txt[[1]])
url = "https://www.quora.com/Can-RCB-win-IPL-2018"
page = read_html(url)
txt = html_nodes(page,'.ui_qtext_rendered_qtext')
txt
xmlTreeParse(txt[[1]])
txt4 = html_text(txt)
txt4
length(txt5)
url = "https://www.quora.com/Can-RCB-win-IPL-2018"
page = read_html(url)
txt = html_nodes(page,'.ui_qtext_rendered_qtext')
# Check one node
xmlTreeParse(txt[[1]])
txt5 = html_text(txt)
length(txt5)
write.csv(txt5,'txt5.txt', row.names = F)
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
#Twitter
api_key <- '4TrCFhHSDkdpA11s7uw6BS9VL'
api_secret <- 'etj2qeBWIThn46JBXBCo07mLiSlDgyFZ4jRvwGLDWfaLhQIuHD'
access_token <- '1902625928-QyInJOhZFUo4B98wSDMDYd2KzvxuwOvXxARGrXW'
access_token_secret <- 'sG5UxgoABRLttHE1VhtXQCBjH6NBKdr1cP86C2SiRHYSK'
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
tweets <- searchTwitter('$RCBTweets',n=2000, lang = 'en')
length(tweets)
api_key <- '4TrCFhHSDkdpA11s7uw6BS9VL'
api_secret <- 'etj2qeBWIThn46JBXBCo07mLiSlDgyFZ4jRvwGLDWfaLhQIuHD'
access_token <- '1902625928-QyInJOhZFUo4B98wSDMDYd2KzvxuwOvXxARGrXW'
access_token_secret <- 'sG5UxgoABRLttHE1VhtXQCBjH6NBKdr1cP86C2SiRHYSK'
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
tweets <- searchTwitter('$RCBTweets',n=2000, lang = 'en')
library(twitteR)
setup_twitter_oauth(api_key,api_secret,access_token, access_token_secret)
tweets <- searchTwitter('$RCBTweets',n=1000, lang = 'en')
tweets <- searchTwitter('$IPL2018',n=2000, lang = 'en')
tweets <- searchTwitter('@RCBTweets',n=2000, lang = 'en')
length(tweets)
tweets
tweets <- searchTwitter('#RCB',n=2000, lang = 'en')
tweets
datafile1 <- read.csv("C:/Users/nkohle/Documents/ISB/TERM1/Practicum/t/txt1.txt")
datafile2 <- read.csv("C:/Users/nkohle/Documents/ISB/TERM1/Practicum/t/txt2.txt")
datafile3 <- read.csv("C:/Users/nkohle/Documents/ISB/TERM1/Practicum/t/txt3.txt")
datafile4 <- read.csv("C:/Users/nkohle/Documents/ISB/TERM1/Practicum/t/txt4.txt")
datafile5 <- read.csv("C:/Users/nkohle/Documents/ISB/TERM1/Practicum/t/txt5.txt")
datafile <- rbind(datafile1, datafile2, datafile3, datafile4, datafile5)
write.csv(datafile,'rcb.txt', row.names = F)
nokia = readLines('C:/Users/nkohle/Documents/ISB/TERM1/Practicum/t/rcb.txt')
# a0 = readLines(file.choose())    # alternately, do this to read file from local machine
text <- nokia
text  =  gsub("<.*?>", " ", text)              # regex for removing HTML tags
length(text)    # 120 documents
str(text)
class(text)
require(tibble)
textdf = data_frame(text = text) # yields 120x1 tibble. i.e., each doc = 1 row here.
textdf %>% unnest_tokens(word, text)   # try ?unnest_tokens
nokia_words = textdf %>%
unnest_tokens(word, text) %>%  # tokenized words in df in 'word' colm
count(word, sort = TRUE) %>%   # counts & sorts no. of occurrences of each item in 'word' column
rename(count = n)      # renames the count column from 'n' (default name) to 'count'.
nokia_words %>% head(., 10) # view top 10 rows in nokia-words df
nokia_new = textdf %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE) %>%
rename(count = n) %>%
anti_join(stop_words)   # try ?anti_join
nokia_new %>% head(., 10)
# First, build a datafame
tidy_nokia <- textdf %>%
unnest_tokens(word, text) %>%     # word tokenization
anti_join(stop_words)    # run ?join::dplyr
tidy_nokia %>%
count(word, sort = TRUE) %>%
filter(n > 20) %>%   # n is wordcount colname.
mutate(word = reorder(word, n)) %>%  # mutate() reorders columns & renames too
ggplot(aes(word, n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip()
tidy_nokia = textdf %>%
mutate(doc = row_number()) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
group_by(doc) %>%
count(word, sort=TRUE)
tidy_nokia
nokia_dtm = tidy_nokia %>%
cast_dtm(doc, word, n)
nokia_dtm <- tidy_nokia %>% cast_sparse(doc, word, n)
nokia_dtm[1:6,1:6]
nokia_dtm_idf = tidy_nokia %>%
group_by(doc) %>%
count(word, sort=TRUE) %>% ungroup() %>%
bind_tf_idf(word, doc, nn) %>%
cast_sparse(doc, word, tf_idf)
nokia_dtm_idf[1:8, 1:8]   # view a few rows
textdf_doc = textdf %>%
mutate(doc = row_number()) %>%   # creates an id number for each row, i.e., doc.
select(doc, text)
# group_by(doc)   # internally groups docs together
textdf_doc
textdf_word = textdf_doc %>%   # using textdf_doc and not textdf_sent!
unnest_tokens(word, text) %>%
group_by(doc) %>%
mutate(word_id = row_number()) %>%
summarise(word_doc = max(word_id)) %>%
select(doc, word_doc)
textdf_word[1:10,]
